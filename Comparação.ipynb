{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparação"
      ],
      "metadata": {
        "id": "w0uaP8N3ddEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "%pip install -U tensorflow tensorflow_hub sentence-transformers openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmlFxODdO5xX",
        "outputId": "3ccff8ee-74e3-44d4-c48d-17f2a5866e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.34.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.17.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nkICUIW6tNuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0dc548-c86c-4c5a-ee0a-e19bdd2b2a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# determina os caminhos das pastas que contém os arquivos de interesse\n",
        "project_folder = \"/content/drive/Shareddrives/Projeto de Estatística/\"\n",
        "\n",
        "article_folder = project_folder + \"TXTs Artigos (Completos)/\"\n",
        "abstract_folder = project_folder + \"TXTs Abstracts/\"\n",
        "\n",
        "ab_OG_folder = abstract_folder + \"ORIGINAL/\"\n",
        "\n",
        "ab_Bard_folder = abstract_folder + \"BARD/\"\n",
        "ab_Bing_folder = abstract_folder + \"BING/\"\n",
        "ab_GPT_folder = abstract_folder + \"CHATGPT/\"\n",
        "\n",
        "# cria listas com os nomes dos arquivos de interesse\n",
        "article_files = []\n",
        "ab_OG_files = []\n",
        "\n",
        "ab_Bard_files = []\n",
        "ab_Bing_files = []\n",
        "ab_GPT_files = []\n",
        "\n",
        "# preenchendo as listas\n",
        "for i in range(1, 151):\n",
        "  num = \"\"\n",
        "  if(i < 100): num += \"0\"\n",
        "  if(i < 10): num += \"0\"\n",
        "  num += str(i)\n",
        "\n",
        "  article_files.append(article_folder + \"No_Abstract\" + num + \".txt\")\n",
        "  ab_OG_files.append(ab_OG_folder + \"Abstract_OG\" + num + \".txt\")\n",
        "\n",
        "  ab_Bard_files.append(ab_Bard_folder + \"Abstract_BARD\" + num + \".txt\")\n",
        "  ab_Bing_files.append(ab_Bing_folder + \"Abstract_BING\" + num + \".txt\")\n",
        "  ab_GPT_files.append(ab_GPT_folder + \"Abstract_GPT\" + num + \".txt\")"
      ],
      "metadata": {
        "id": "eVMc1FI9Nana"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OG_article_texts = []\n",
        "OG_abstract_texts = []\n",
        "Bard_texts = []\n",
        "Bing_texts = []\n",
        "GPT_texts = []\n",
        "\n",
        "for i in range(150):\n",
        "  file_OG_article = open(article_files[i])\n",
        "  OG_article_texts.append(file_OG_article.read())\n",
        "  file_OG_article.close()\n",
        "\n",
        "  file_OG_abstract = open(ab_OG_files[i])\n",
        "  OG_abstract_texts.append(file_OG_abstract.read())\n",
        "  file_OG_abstract.close()\n",
        "\n",
        "  file_Bard = open(ab_Bard_files[i])\n",
        "  Bard_texts.append(file_Bard.read())\n",
        "  file_Bard.close()\n",
        "\n",
        "  file_Bing = open(ab_Bing_files[i])\n",
        "  Bing_texts.append(file_Bing.read())\n",
        "  file_Bing.close()\n",
        "\n",
        "  file_GPT = open(ab_GPT_files[i])\n",
        "  GPT_texts.append(file_GPT.read())\n",
        "  file_GPT.close()"
      ],
      "metadata": {
        "id": "ilNUrQDgKs07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-processamento"
      ],
      "metadata": {
        "id": "25xAke3CjDiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def cos_sim(sentence1_emb, sentence2_emb):\n",
        "    \"\"\"\n",
        "    Similaridade de cosseno entre duas colunas de embeddings de sentenças\n",
        "\n",
        "    Argumentos:\n",
        "      sentence1_emb: coluna do embedding de sentence1\n",
        "      sentence2_emb: coluna do embedding de sentence2\n",
        "\n",
        "    Returns:\n",
        "      A similaridade de cosseno das linhas entre as duas colunas\n",
        "\n",
        "      Se sentence1_emb=[a,b,c] and sentence2_emb=[x,y,z]\n",
        "      Então o resultado é [cosine_similarity(a,x), cosine_similarity(b,y), cosine_similarity(c,z)]\n",
        "    \"\"\"\n",
        "    cos_sim = cosine_similarity(sentence1_emb, sentence2_emb)\n",
        "    return np.diag(cos_sim)"
      ],
      "metadata": {
        "id": "Y-m_pNULjK5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparação\n",
        "\n",
        "[Semantic Textual Similarity](https://towardsdatascience.com/semantic-textual-similarity-83b3ca4a840e)\n",
        "\n",
        "Universal Semantic Encoder (USE) escolhido por ser o mais preciso que consegue lidar com textos longos.\n"
      ],
      "metadata": {
        "id": "gMiqlkx0pste"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Load the pre-trained model\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    # Control GPU memory usage\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')"
      ],
      "metadata": {
        "id": "AP5Rw163zoFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts_list = [OG_abstract_texts, Bard_texts, Bing_texts, GPT_texts]\n",
        "\n",
        "# com isso definimos\n",
        "#   0 = OG\n",
        "#   1 = Bard\n",
        "#   2 = Bing\n",
        "#   3 = GPT\n",
        "\n",
        "\n",
        "article_embeddings = embed(OG_article_texts)\n",
        "abstracts_embeddings = [embed(abstracts) for abstracts in abstracts_list]"
      ],
      "metadata": {
        "id": "W64qTzAAUN2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = []\n",
        "\n",
        "for i in range(4):\n",
        "    abstract_set = abstracts_embeddings[i]\n",
        "    similarities_set = cosine_similarity(article_embeddings, abstract_set)\n",
        "    similarities.append(similarities_set)\n",
        "\n",
        "# 'similarities' contém uma lista de 4 matrizes de similaridade, uma equivalente a cada abstract\n",
        "# similarities[i][j][j] diz a similaridade do abstract j+1 do tipo i com o artigo j+1 correspondente"
      ],
      "metadata": {
        "id": "RURGYTRqcFX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salvando os Resultados"
      ],
      "metadata": {
        "id": "ZNWn05K9fb4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "results_folder = project_folder + \"Análise/Resultados\"\n",
        "\n",
        "if not os.path.exists(results_folder):\n",
        "    os.makedirs(results_folder)\n",
        "\n",
        "results_folder += '/'"
      ],
      "metadata": {
        "id": "xT9RjYwHgLTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# para os abstracts originais\n",
        "\n",
        "filename = results_folder + \"Comparações_OG.txt\"\n",
        "OG = open(filename, \"w\")\n",
        "\n",
        "for i in range(150):\n",
        "  value = \"{:,.6f}\".format(similarities[0][i][i]).replace(\".\", \",\")\n",
        "  OG.write(value + \"\\n\")\n",
        "\n",
        "OG.close()"
      ],
      "metadata": {
        "id": "9yIm3ksnffWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# para os abstracts Bard\n",
        "\n",
        "filename = results_folder + \"Comparações_BARD.txt\"\n",
        "BARD = open(filename, \"w\")\n",
        "\n",
        "for i in range(150):\n",
        "  value = \"{:,.6f}\".format(similarities[1][i][i]).replace(\".\", \",\")\n",
        "  BARD.write(value + \"\\n\")\n",
        "\n",
        "BARD.close()"
      ],
      "metadata": {
        "id": "8n2hweAKfjuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#para os abstracts Bing\n",
        "\n",
        "filename = results_folder + \"Comparações_BING.txt\"\n",
        "BING = open(filename, \"w\")\n",
        "\n",
        "for i in range(150):\n",
        "  value = \"{:,.6f}\".format(similarities[2][i][i]).replace(\".\", \",\")\n",
        "  BING.write(value + \"\\n\")\n",
        "\n",
        "BING.close()"
      ],
      "metadata": {
        "id": "PfUgx7Xhf4N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# para os abstracts GPT\n",
        "\n",
        "filename = results_folder + \"Comparações_GPT.txt\"\n",
        "GPT = open(filename, \"w\")\n",
        "\n",
        "for i in range(150):\n",
        "  value = \"{:,.6f}\".format(similarities[3][i][i]).replace(\".\", \",\")\n",
        "  GPT.write(value + \"\\n\")\n",
        "\n",
        "GPT.close()"
      ],
      "metadata": {
        "id": "xNrjbUOQf93x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}